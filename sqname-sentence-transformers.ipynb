{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipebook-Approved SQName Generator\n",
    "The following expedition explores [Sentence Transformers](https://huggingface.co/sentence-transformers) to suggest a **Recipebook-Approved SQName** which contains the _us-gaap_ namespace for a given Name to prevent redundant generation of derived SQNames from custom tickers. This is an effort towards improving comparability in financial reports. Sentence Transformers are state-of-the-art models for semantic textual similarity. It is meant to be representative of a high-dimensional embedding model that can effectively capture semantic similarity between Names and existing SQNames. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning a Sentence Transformer\n",
    "The Hugging Face Sentence Transformer is leveraged with a powerful yet lightweight pre-trained model, and then fine-tuned with the dataset provided to serve this specific use-case. \n",
    "\n",
    "### Data Pre-processing \n",
    "To truly capture the semantics of a name, noise/additional characters are removed and standard preprocessing techniques are employed. \n",
    "- Punctuation and Special Characters are removed\n",
    "- Sentence is tokenized\n",
    "- Converted to lowercase \n",
    "- Stopwords are removed\n",
    "- Lemmatization (root words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/meg-kumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/meg-\n",
      "[nltk_data]     kumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/meg-\n",
      "[nltk_data]     kumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    sentence = sentence.translate(translator)\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Join the tokens back into a sentence\n",
    "    preprocessed_sentence = ' '.join(tokens)\n",
    "    return preprocessed_sentence\n",
    "\n",
    "def camel_case_split(str):\n",
    "    return \" \".join(re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data and store it in the Names column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [net income net income, net income loss attrib...\n",
       "1    [cash flow investing activity capital expendit...\n",
       "2    [equity apache shareholder equity, stockholder...\n",
       "3    [net income tax, provision benefit income tax,...\n",
       "4    [equity total liability equity, liability equi...\n",
       "Name: Names, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = \"data/NLP Task Dataset - 23rd June 2023.xlsx\"\n",
    "df = pd.read_excel(data)\n",
    "df['Names'] = df['Names'].apply(lambda x: x.split('&&'))\n",
    "df['Names'] = df['Names'].apply(lambda x: [preprocess_sentence(name) for name in x])\n",
    "df['Names'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQName (URI)\n",
    "This class implements SQName as a Unique Resource Identifier (URI) for the financial concepts. This will be handy since SQNames will be used often and this will help it scale well instead of constantly formatting a string to extract information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQName(str):\n",
    "    def __init__(\n",
    "        self,\n",
    "        uri: str | None = None,\n",
    "    ):\n",
    "        if isinstance(uri, str):\n",
    "            atoms = uri.rsplit(\"_\", 1)\n",
    "            namespace, tag = atoms\n",
    "\n",
    "        namespace, tag = str(namespace), str(tag)\n",
    "\n",
    "        if any(atom == \"\" for atom in (namespace, tag)):\n",
    "            raise ValueError(f\"Invalid URI: namespace = {namespace}, tag = {tag}\")\n",
    "        self._namespace = namespace\n",
    "        self._tag = tag\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def namespace(self):\n",
    "        \"\"\"Get the URI namespace.\"\"\"\n",
    "        return self._namespace\n",
    "\n",
    "    @property\n",
    "    def tag(self):\n",
    "        \"\"\"Get the URI tag.\"\"\"\n",
    "        return self._tag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Finetuning \n",
    "- (tag, name) Pairs of positive examples - text similar to each other - are used to fine-tune Transformer Models. This needs to be extracted from the dataset. The recipe-book is considered to be a SQNames with _us-gaap_ as their namespace. The positive examples in the dataset are selected by pairing the tag of these SQNames with the each pre-processed name extracted from Names. \n",
    "- (name1, name2) Additionally, a sampling factor of 0.5 (this is scaled down for the scope of this experiment) multiplied with the Frequency of the item is used to also sample pairs from within the Names list, since they are also similar to _each other_. The sampling is used since considering every combination pair would be computationally expensive. The Frequency is used to represent higher frequency SQNames with a greater weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15815it [00:01, 9522.36it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('net income loss', 'net income net income'),\n",
       " ('net income loss', 'net income loss attributable parent'),\n",
       " ('net income loss', 'net earnings'),\n",
       " ('net income loss', 'net earnings attributable mohawk industry inc'),\n",
       " ('net income loss', 'net loss'),\n",
       " ('net income loss', 'net income loss'),\n",
       " ('net income loss', 'net income omnicom group inc'),\n",
       " ('net income loss', 'net income omnicom group inc'),\n",
       " ('net income loss', 'net income attributed omnicom group inc'),\n",
       " ('net income loss', 'net income loss attributable groupon inc')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "pairs = []\n",
    "SAMPLING_FACTOR = 0.5\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    entity = SQName(row[\"SQName\"])\n",
    "    if entity.namespace == \"us-gaap\":\n",
    "        # Pairs of positive examples (tag, name)\n",
    "        entity_name = preprocess_sentence(camel_case_split(entity.tag))\n",
    "        for name in row[\"Names\"]:\n",
    "            pairs.append((entity_name, name))\n",
    "        # Pairs of positive examples (name1, name2) sampled from the same tag\n",
    "        k_samples = int(row[\"Frequency\"] * SAMPLING_FACTOR)\n",
    "        for _ in range(k_samples):\n",
    "            pair = random.choices(row[\"Names\"], k=2)\n",
    "            pairs.append(pair)\n",
    "pairs[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformer \n",
    "Pre-trained model [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function - Multiple Negatives Ranking Loss\n",
    "The [MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss) is used if you only have positive pairs, for example, only pairs of similar texts like pairs of paraphrases, pairs of duplicate questions - in this case pairs of (SQName tag, and Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS CELL IF YOU WANT TO REPRODUCE THE RESULTS, IT TAKES A LONG TIME TO RUN\n",
    "# THE PATH TO MODEL HAS THE MODEL YOU WILL NEED, YOU CAN IMPORT IT DIRECTLY\n",
    "\n",
    "train_examples = [InputExample(texts=[pair[0], pair[1]]) for pair in pairs]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10) \n",
    "PATH_TO_MODEL = \"data/sqname_fine_tuned_model_with_sampling.bin\"\n",
    "model.save(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"data/sqname_fine_tuned_model_with_sampling.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(PATH_TO_MODEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipebook as embeddings\n",
    "Using the model above, the recipebook is augmented with the encoded embeddings from the fine-tuned model. This takes the tag from the SQNames with the _us-gaap_ namespacce, preprocesses them, and encodes them into a 384 dimensional vector output from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[-0.01032155, 0.060474418, 0.053759195, -0.01...\n",
       "1    [[0.00035809193, 0.0012922849, -0.021274941, 0...\n",
       "2    [[-0.013603674, -0.06935441, -0.09837998, 0.02...\n",
       "3    [[-0.017173182, -0.01779158, 0.016376264, -0.0...\n",
       "4    [[-0.00584583, -0.082059816, -0.032502823, 0.0...\n",
       "Name: code_embedding, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_df = df[df.apply(lambda x: \"us-gaap\" in x[\"SQName\"], axis=1)]\n",
    "recipe_df[\"text\"]  = recipe_df[\"SQName\"].apply(lambda x: preprocess_sentence(camel_case_split(SQName(x).tag)))\n",
    "recipe_df['code_embedding'] = recipe_df['text'].apply(lambda x: model.encode([x]))\n",
    "recipe_df['code_embedding'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity - Vector search\n",
    "The recipebook is searched for the SQName closest to a given code query using similarity code search. Similarity between vectors is sought through the cosine similarity index. \n",
    "- This is currently sorted by the similarity score. \n",
    "- The similarity is filtered to be > 0.8\n",
    "- It could in the future also be sorted by Frequency, once it is sorted by Similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def search_functions(recipe_df, code_query, n=5):\n",
    "   embedding = model.encode([preprocess_sentence(code_query)])\n",
    "   recipe_df['similarities'] = recipe_df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "   # filter out the ones with similarity < 0.8\n",
    "   copy = recipe_df[recipe_df[\"similarities\"] > 0.8].dropna()\n",
    "   res = copy.sort_values(['similarities'], ascending=[False]).head(n)\n",
    "   suggestions = res[\"SQName\"].tolist()\n",
    "   if len(suggestions):\n",
    "       return suggestions\n",
    "   else:\n",
    "         return \"You are free to create your own extension, there isn't a direct match!\"\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Three examples for randomly sampled names from the dataset. \n",
    "1. Amount of judgment or settlement awarded to (against) the entity in respect of litigation__ which will be fully absorbed by the Company's insurance carrier\n",
    "2. Income derived from investments and income not otherwise specified in the income statement. Interest income represents earnings which reflect the time value of money or transactions in which the payments are for the use or forbearance of money. Dividend income represents a distribution of earnings to shareholders by investee companies\n",
    "3. Food__ beverage__ racing and other revenue includes revenue from food and beverage sales__ racing revenue earned from pari-mutuel wagering on live harness racing and simulcast signals from other tracks and miscellaneous income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us-gaap_LitigationSettlementAmount',\n",
       " 'us-gaap_LitigationSettlementAmountAwardedFromOtherParty',\n",
       " 'us-gaap_LitigationSettlementExpense',\n",
       " 'us-gaap_PaymentsForLegalSettlements',\n",
       " 'us-gaap_GainLossRelatedToLitigationSettlement']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions(recipe_df, \"Amount of judgment or settlement awarded to (against) the entity in respect of litigation__ which will be fully absorbed by the Company's insurance carrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us-gaap_InvestmentIncomeInterestAndDividend',\n",
       " 'us-gaap_OtherInterestAndDividendIncome',\n",
       " 'us-gaap_InterestAndDividendIncomeOperating',\n",
       " 'us-gaap_InvestmentIncomeDividend',\n",
       " 'us-gaap_DividendIncomeOperating']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions(recipe_df, \"Income derived from investments and income not otherwise specified in the income statement. Interest income represents earnings which reflect the time value of money or transactions in which the payments are for the use or forbearance of money. Dividend income represents a distribution of earnings to shareholders by investee companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us-gaap_FoodAndBeverageRevenue']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions(recipe_df, \"Food__ beverage__ racing and other revenue includes revenue from food and beverage sales__ racing revenue earned from pari-mutuel wagering on live harness racing and simulcast signals from other tracks and miscellaneous income\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query also works for Names not previously in the dataset, since it is not a direct lookup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are free to create your own extension, there isn't a direct match!\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions(recipe_df, \"Money spent on restaurants and travel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us-gaap_CapitalizedComputerSoftwareGross', 'us-gaap_PaymentsForSoftware']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions(recipe_df, \"Equipment and software includes the cost of computer hardware and software__ and the cost of equipment used in the Company's operations\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analysis\n",
    "Sampling 500 \"custom\" ticker items and suggesting recipebook-approved SQNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = df[df.apply(lambda x: \"us-gaap\" not in x[\"SQName\"], axis=1)].sample(500)\n",
    "random_df[\"text\"]  = random_df[\"SQName\"].apply(lambda x: preprocess_sentence(camel_case_split(SQName(x).tag)))\n",
    "random_df[\"Recipebook SQName\"] = random_df[\"text\"].apply(lambda x: search_functions(recipe_df, x, n=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAMPLE_XLSX = \"data/sqname_suggestions_sample.xlsx\"\n",
    "random_df.to_excel(PATH_TO_SAMPLE_XLSX, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The predictions for **Recipebook SQName** are in the _us-gaap_ namespace. They are filtered by a minimum of 0.8 similarity. If the list turns out to be empty, it might indicate that there are no obvious matches from the recipebook, and it could be considered a **valid** or an **appropriate** Extension. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
